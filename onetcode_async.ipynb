{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e50262-3129-4df6-b2ae-137c58c615f0",
   "metadata": {},
   "source": [
    "# ***Importing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bb4a6-5135-4ffd-a7dc-5f130c09579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup # Using lxml instead\n",
    "from lxml import html\n",
    "# import requests # Using httpx instead\n",
    "# import aiohttp # Using httpx instead\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import numpy as py\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "directory = 'G:\\\\Other computers\\\\My Laptop\\\\Script\\\\Web Scraping Project\\\\Tutorials\\\\Asyncio'\n",
    "os.chdir(directory)\n",
    "\n",
    "df = pd.read_csv(\"df_nocode.csv\")\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024962ac-c2a4-41c7-b5e7-5a061c6643b9",
   "metadata": {},
   "source": [
    "# ***Scrape (httpx, lxml)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599eb8e-52ec-4939-adb5-a1f7b222d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow running in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Async Functions\n",
    "async def fetch(session, url, row_num, attempts=9, delay=5):\n",
    "    for attempt in range(attempts):\n",
    "        resp = await session.get(url)\n",
    "        if resp.status_code == 200:\n",
    "            return resp\n",
    "        await asyncio.sleep(delay)\n",
    "        print(f\"Reconnecting at {row_num}... Attempt {attempt+1}/10\")\n",
    "    print(f\"Couldn't connect after {attempts+1} attempts in {row_num}\")\n",
    "    return None\n",
    "\n",
    "async def scrape(session, url, row_num):\n",
    "    page = await fetch(session, url, row_num)\n",
    "    if page:\n",
    "        raw_code = html.fromstring(page.text)\n",
    "        code = raw_code.xpath('//*[@id=\"description\"]/div[1]/p[2]/text()')\n",
    "        if code:\n",
    "            return code\n",
    "        print(f'At {row_num}, code was not found')\n",
    "        return None    \n",
    "    print(f'At {row_num}, fetched page was successful but could not be processed')\n",
    "    return None\n",
    "        \n",
    "async def processing(session, row_num):\n",
    "    url = df.loc[row_num, 'esco_link']\n",
    "    posted_link = urlparse(url).path.split('/')[-1]\n",
    "    \n",
    "    #-# Get the link (because requests cannot get links that are essentially shortened/changed like bit.ly):\n",
    "    if 'occupation' in url:\n",
    "        esco_link = f\"https://esco.ec.europa.eu/en/classification/occupation?uri=http%3A%2F%2Fdata.europa.eu%2Fesco%2Foccupation%2F{posted_link}\"\n",
    "    else:\n",
    "        esco_link = f\"https://esco.ec.europa.eu/en/classification/occupation?uri=http%3A%2F%2Fdata.europa.eu%2Fesco%2Fisco%2F{posted_link}\"\n",
    "    \n",
    "    return await scrape(session, esco_link, row_num)\n",
    "\n",
    "async def main(batches = 5, delay = 2):\n",
    "    total_url = 21\n",
    "    code_list = []\n",
    "\n",
    "    # PUT A LOAD FILE TRY-EXCEPT CODE HERE #\n",
    "    # PUT A LOAD FILE TRY-EXCEPT CODE HERE #\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=100) as session:\n",
    "        for batch_start in range(0, total_url, batches):\n",
    "            batch_end = min(batch_start + batches, total_url)\n",
    "            \n",
    "            tasks = [processing(session, row_num) for row_num in range(batch_start, batch_end)]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            code_list.extend(results)\n",
    "            print(f\"Batches {batch_start} to {batch_end} done!\")\n",
    "\n",
    "            # PUT A SAVE FILE CODE HERE #\n",
    "            print(code_list)\n",
    "            # PUT A SAVE FILE CODE HERE #\n",
    "        \n",
    "            await asyncio.sleep(delay)\n",
    "            \n",
    "    return code_list\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.perf_counter()\n",
    "await main()\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time elapsed: {end_time - start_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
